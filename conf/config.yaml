hydra:
  run:
    dir: outputs/${hydra.job.name}/${now:%Y-%m-%d_%H-%M-%S}
  sweep:
    dir: outputs/multirun/${hydra.job.name}/${data_name}/${now:%Y-%m-%d}/${now:%H-%M-%S}
    subdir: ${hydra.job.num}

method: 'cl-vime' # sl-only, semi-only, self-semi-sl, cl-vime

data_name: 'mnist' # iris, wine, boston, mnist
label_data_rate: 0.1
self_epochs: 10
semi_max_iter: 1000
batch_size: 128
test_batch_size: 1024
p_m: 0.3  # Corruption(mask) probability for self-supervised learning
c: 0.2 # Corruption(noise) probability for self-supervised learning
k: 3  # Number of augmented samples
alpha: 2.0  # Hyper-parameter to control the weights of feature and mask losses
beta: 1.0 # Hyperparameter to control supervised and unsupervised losses
temperature: 0.14 # Hyperparameter to control the weights of InfoNCE loss(contrastive loss)

early_stopping_patience: 10

direction: maximize # maximize or minimize
n_trials: 30 # Number of times to search hyperparameter by using optuna
seed: 42